{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a518960",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.12.0\n",
    "!pip install ampligraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490ca5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d44eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import ampligraph\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from ampligraph.datasets import load_fb15k_237\n",
    "from ampligraph.evaluation.protocol import train_test_split_no_unseen\n",
    "from ampligraph.evaluation.metrics import mr_score, mrr_score, hits_at_n_score\n",
    "\n",
    "from ampligraph.datasets import load_wn18\n",
    "from ampligraph.latent_features import ScoringBasedEmbeddingModel\n",
    "from ampligraph.evaluation import mrr_score, hits_at_n_score\n",
    "from ampligraph.latent_features.loss_functions import get as get_loss\n",
    "from ampligraph.latent_features.regularizers import get as get_regularizer\n",
    "import tensorflow as tf\n",
    "\n",
    "from ampligraph.latent_features.layers.scoring import TransE, ComplEx, HolE, DistMult\n",
    "from ampligraph.utils import save_model, restore_model\n",
    "from ampligraph.evaluation import select_best_model_ranking\n",
    "from ampligraph.latent_features.models.ScoringBasedEmbeddingModel import ScoringBasedEmbeddingModel\n",
    "from ampligraph.utils import save_model, restore_model\n",
    "import gradio as gr\n",
    "import tensorflow as tf\n",
    "\n",
    "print('TensorFlow  version: {}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f87820",
   "metadata": {},
   "outputs": [],
   "source": [
    "ComplEx_model_path = '/content/drive/MyDrive/kg_models/ComplEx.pkl'\n",
    "RotatE_model_path = '/content/drive/MyDrive/kg_models/RotatE.pkl'\n",
    "TransE_model_path = '/content/drive/MyDrive/kg_models/TransE.pkl'\n",
    "\n",
    "restored_ComplEx_model = restore_model(model_name_path=ComplEx_model_path)\n",
    "restored_RotatE_model = restore_model(model_name_path=RotatE_model_path)\n",
    "restored_TransE_model = restore_model(model_name_path=TransE_model_path)\n",
    "\n",
    "df=pd.read_csv('all_triplets.csv',index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd20bf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_train, X_valid = train_test_split_no_unseen(df.values, 500, seed=0)\n",
    "X_train, X_test = train_test_split_no_unseen(test_train, 1000, seed=0)\n",
    "\n",
    "print('Total triples:', df.shape)\n",
    "print('Size of train:', X_train.shape)\n",
    "print('Size of valid:', X_valid.shape)\n",
    "print('Size of test:', X_test.shape)\n",
    "\n",
    "X = {\n",
    "    'train': X_train,\n",
    "    'valid': X_valid,\n",
    "    'test':  X_test,\n",
    "}\n",
    "\n",
    "filter_dict = {'test': np.concatenate((X['train'], X['valid'], X['test']))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729bc7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_data, filter_dict):\n",
    "    ranks = model.evaluate(test_data, use_filter=filter_dict, corrupt_side='s,o')\n",
    "    return {\n",
    "        \"MRR\": mrr_score(ranks),\n",
    "        \"Hits@1\": hits_at_n_score(ranks, n=1),\n",
    "        \"Hits@10\": hits_at_n_score(ranks, n=10),\n",
    "        \"Hits@50\": hits_at_n_score(ranks, n=50),\n",
    "        \"Hits@100\": hits_at_n_score(ranks, n=100)\n",
    "    }\n",
    "results = {\n",
    "    \"ComplEx\": evaluate_model(restored_ComplEx_model, X['test'], filter_dict),\n",
    "    \"RotatE\": evaluate_model(restored_RotatE_model, X['test'], filter_dict),\n",
    "    \"TransE\": evaluate_model(restored_TransE_model, X['test'], filter_dict)\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(results).T.reset_index().rename(columns={\"index\": \"Model\"})\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9938b6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare random test subset\n",
    "np.random.seed(98)\n",
    "testing_subset = X['test'][np.random.choice(len(X['test']), size=10, replace=False)]\n",
    "entities = sorted(set(df['subject']) | set(df['object']))\n",
    "def get_tail_prediction_ranks_all_models(test_triples, models_dict, entities, filter_dict, top_k=10):\n",
    "    results = []\n",
    "    known_triples = set(tuple(triple) for triple in filter_dict['test'])\n",
    "\n",
    "    for h, r, true_t in test_triples:\n",
    "        row = {\n",
    "            \"Head\": h,\n",
    "            \"Relation\": r,\n",
    "            \"True Tail\": true_t\n",
    "        }\n",
    "\n",
    "        for model_name, model in models_dict.items():\n",
    "            candidate_triplets = [(h, r, t) for t in entities]\n",
    "            filtered_candidates = [\n",
    "                (h_tmp, r_tmp, t_tmp)\n",
    "                for (h_tmp, r_tmp, t_tmp) in candidate_triplets\n",
    "                if (h_tmp, r_tmp, t_tmp) not in known_triples or t_tmp == true_t\n",
    "            ]\n",
    "\n",
    "            scores = model.predict(filtered_candidates)\n",
    "            sorted_indices = np.argsort(scores)[::-1]\n",
    "            ranked_tails = [filtered_candidates[i][2] for i in sorted_indices]\n",
    "            rank = ranked_tails.index(true_t) + 1 if true_t in ranked_tails else \"Not in top\"\n",
    "\n",
    "            row[f\"Rank ({model_name})\"] = rank\n",
    "\n",
    "        results.append(row)\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "models_dict = {\n",
    "    \"ComplEx\": restored_ComplEx_model,\n",
    "    \"RotatE\": restored_RotatE_model,\n",
    "    \"TransE\": restored_TransE_model\n",
    "}\n",
    "\n",
    "df_multi_rank = get_tail_prediction_ranks_all_models(testing_subset, models_dict, entities, filter_dict, top_k=10)\n",
    "df_multi_rank"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
